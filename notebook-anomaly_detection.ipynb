{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ea5f1f-3f56-4d50-af00-df54361a5196",
   "metadata": {},
   "source": [
    "# Anomaly Detection: Visual Quality Inspection in the Industrial Domain\n",
    "\n",
    "Manual anomaly detection is time and labor-intensive, which limits its applicability on large volumes of data that are typical in industrial settings. Application of artificial intelligence and machine learning is transforming Industrial Internet of Things (IIoT) segments by enabling higher productivity, better insights, less downtime, and superior product quality.\n",
    "\n",
    "The goal of this anomaly detection reference use case is to provide AI-powered visual quality inspection on high resolution input images by identifying rare, abnormal events such as defects in a part being manufactured on an industrial production line. Use this reference solution as-is on your dataset, curate it to your needs by fine-tuning the models, change configurations to get improved performance, and modify it to meet your productivity and performance goals by making use of the modular architecture and realize superior performance using the Intel optimized software packages and libraries for Intel hardware that are built into the solution.\n",
    " \n",
    "\n",
    "## **Table of Contents**\n",
    "- [Technical Overview](#technical-overview)\n",
    "    - [DataSet](#DataSet)\n",
    "- [Validated Hardware Details](#validated-hardware-details)\n",
    "- [Software Requirements](#software-requirements)\n",
    "- [How it Works?](#how-it-works)\n",
    "- [Get Started](#get-started)\n",
    "    - [Download the Workflow Repository](#Download-the-Workflow-Repository)\n",
    "    - [Download the Transfer Learning Tool](#Download-the-Transfer-Learning-Tool)\n",
    "- [Ways to run this reference use case](#Ways-to-run-this-reference-use-case)\n",
    "    - [Run Using Bare Metal](#run-using-bare-metal) \n",
    "- [Expected Output](#expected-output)\n",
    "- [Summary and Next Steps](#summary-and-next-steps)\n",
    "    - [Adopt to your dataset](#adopt-to-your-dataset)\n",
    "    - [Adopt to your model](#adopt-to-your-model)\n",
    "- [Learn More](#learn-more)\n",
    "- [Support](#support)\n",
    "\n",
    "<a id=\"technical-overview\"></a> \n",
    "## Technical Overview\n",
    "Classic and modern anomaly detection techniques have certain challenges:\n",
    "\n",
    "- Feature engineering needs to be performed to extract representations from the raw data. Traditional ML techniques rely on hand-crafted features that may not always generalize well to other settings.\n",
    "- Classification techniques require labeled training data, which is challenging because anomalies are typically rare occurrences and obtaining it increases the data collection & annotation effort.\n",
    "- Nature of anomalies can be arbitrary and unknown where failures or defects occur for a variety of unpredictable reasons, hence it may not be possible to predict the type of anomaly.\n",
    "\n",
    "To overcome these challenges and achieve state-of-the-art performance, we present an unsupervised, mixed method end-to-end fine-tuning & inference reference solution for anomaly detection where a model of normality is learned from defect-free data in an unsupervised manner, and deviations from the models are flagged as anomalies. This reference use case is accelerated by Intel optimized software and is built upon easy-to-use Intel Transfer Learning Tool APIs.\n",
    "\n",
    "<a id=\"DataSet\"></a> \n",
    "### DataSet\n",
    "[MVTec AD](https://www.mvtec.com/company/research/datasets/mvtec-ad) is a dataset for benchmarking anomaly detection methods focused on visual quality inspection in the industrial domain. It contains over 5000 high-resolution images divided into ten unique objects and five unique texture categories. Each category comprises a set of defect-free training images and a test set of images with various kinds of defects as well as defect-free images. There are 73 different types of anomalies in the form of defects or structural deviations present in these objects and textures.\n",
    "\n",
    "More information can be in the paper [MVTec AD – A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection](https://www.mvtec.com/fileadmin/Redaktion/mvtec.com/company/research/datasets/mvtec_ad.pdf)\n",
    "\n",
    "![Statistical_overview_of_the_MVTec_AD_dataset](./assets/mvtec_dataset_characteristics.JPG)\n",
    "<br>\n",
    "*Table 1:  Statistical overview of the MVTec AD dataset. For each category, the number of training and test images is given together with additional information about the defects present in the respective test images. [Source](https://www.mvtec.com/fileadmin/Redaktion/mvtec.com/company/research/datasets/mvtec_ad.pdf)*\n",
    "\n",
    "<a id=\"validated-hardware-details\"></a> \n",
    "## Validated Hardware Details\n",
    "There are workflow-specific hardware and software setup requirements depending on how the workflow is run. Bare metal development system and Docker image running locally have the same system requirements. \n",
    "\n",
    "| Recommended Hardware         | Precision  |\n",
    "| ---------------------------- | ---------- |\n",
    "| Intel® 4th Gen Xeon® Scalable Performance processors| float32, bfloat16 |\n",
    "| Intel® 1st, 2nd, 3rd Gen Xeon® Scalable Performance processors| float32 |\n",
    "\n",
    "<a id=\"software-requirements\"></a> \n",
    "## Software Requirements \n",
    "Linux OS (Ubuntu 20.04) is used in this reference solution. Make sure the following dependencies are installed.\n",
    "\n",
    "1. `sudo apt update`\n",
    "1. `sudo apt-get install -y libgl1 libglib2.0-0`\n",
    "1. pip/conda OR python3.9-venv\n",
    "1. git\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e233e40-cb7f-4200-997c-ef659f679972",
   "metadata": {},
   "source": [
    "<a id=\"how-it-works\"></a> \n",
    "## How It Works?\n",
    "\n",
    "This reference use case uses a deep learning based approach, named deep-feature modeling (DFM) and falls within the broader area of out-of-distribution (OOD) detection i.e. when a model sees an input that differs from its training data, it is marked as an anomaly. Learn more about the approach [here.](https://arxiv.org/pdf/1909.11786.pdf) \n",
    "\n",
    "The use case provides 3 options for modeling of the vision subtask:\n",
    "* **Pre-trained backbone:** uses a deep network (ResNet-50v1.5 in this case) that has been pretrained on large visual datasets such as ImageNet\n",
    "* **SimSiam self-supervised learning:** is a contrastive learning method based on Siamese networks. It learns meaningful representation of dataset without using any labels. SimSiam requires a dataloader such that it can produce two different augmented images from one underlying image. The end goal is to train the network to produce same features for both images. It takes a ResNet model as the backbone and fine-tunes the model on the augmented dataset to get closer feature embeddings for the use case. Read more [here.](https://arxiv.org/pdf/2011.10566.pdf)\n",
    "* **Cut-Paste self-supervised learning:** is a contrastive learning method similar to SimSiam but differs in the augmentations used during training. It take a ResNet model as backbone and fine-tunes the model after applying a data augmentation strategy that cuts an image patch and pastes at a random location of a large image. This allows us to construct a high performance model for defect detection without presence of anomalous data. Read more [here.](https://arxiv.org/pdf/2104.04015.pdf)\n",
    "\n",
    "![visual_quality_inspection_pipeline](./assets/visual_quality_inspection_pipeline.JPG)\n",
    "*Figure 1: Visual quality inspection pipeline. Above diagram is an example when using SimSiam self-supervised training.*\n",
    "\n",
    "Training stage only uses defect-free data. Images are loaded using a dataloader and shuffling, resizing & normalization processing is applied. Then one of the above stated transfer learning technique is used to fine-tune a model and extract discriminative features from an intermediate layer. A PCA kernel is trained over these features to reduce the dimension of the feature space while retaining 99% variance. This pre-processing of the intermediate features of a DNN is needed to prevent matrix singularities and rank deficiencies from arising.\n",
    "\n",
    "During inference, the feature from a test image is generated through the same network as before. We then run a PCA transform using the trained PCA kernel and apply inverse transform to recreate original features and generate a feature-reconstruction error score, which is the norm of the difference between the original feature vector and the pre-image of its corresponding reduced embedding. Any image with an anomaly will have a high error in reconstructing original features due to features being out of distribution from the defect-free training set and will be marked as anomaly. The effectiveness of these scores in distinguishing the good images from the anomalous images is assessed by plotting the ROC curve, which is a plot of the true positive rate (TPR) of the classifier against the false positive rate (FPR) as the classification score-threshold is varied. The AUROC metric summarizes this curve between 0 to 1, with 1 indicating perfect classification.\n",
    "\n",
    "\n",
    "\n",
    "**Architecture:**\n",
    "![Visual_quality_inspection_layered_architecture](./assets/Visual_quality_inspection_layered_architecture.JPG)\n",
    "\n",
    "### Highlights of Visual Quality Inspection Reference Use Case\n",
    "- The use case is presented in a modular architecture. To improve productivity and reduce time-to-solution, transfer learning methods are made available through an independent workflow that seamlessly uses Intel Transfer Learning Tool APIs underneath and a config file allows the user to change parameters and settings without having to deep-dive and modify the code.\n",
    "- There is flexibility to select any pre-trained model and any intermediate layer for feature extraction.\n",
    "- The use case is enabled with Intel optimized foundational tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50f7f2",
   "metadata": {},
   "source": [
    "<a id=\"get-started\"></a> \n",
    "## Get Started\n",
    "\n",
    "<a id=\"Download-the-Workflow-Repository\"></a> \n",
    "### Download the Workflow Repository\n",
    "\n",
    "Create a working directory for the reference use case and clone the [Visual Quality Inspection Workflow](https://github.com/intel/visual-quality-inspection) repository into your working directory.\n",
    "```\n",
    "git clone https://github.com/intel/visual-quality-inspection\n",
    "cd visual-quality-inspection\n",
    "```\n",
    "\n",
    "<a id=\"Download-the-Transfer-Learning-Tool\"></a> \n",
    "### Download the Transfer Learning Tool\n",
    "```\n",
    "git submodule update --init --recursive\n",
    "export PYTHONPATH=transfer-learning/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac9bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONPATH=transfer-learning/\n",
    "import os\n",
    "print(os.environ[\"PYTHONPATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9dd440",
   "metadata": {},
   "source": [
    "<a id=\"Ways-to-run-this-reference-use-case\"></a> \n",
    "# Ways to run this reference use case\n",
    "\n",
    "This reference kit offers three options for running the fine-tuning and inference processes:\n",
    "\n",
    "- Docker\n",
    "- Argo Workflows on K8s Using Helm\n",
    "- [Bare Metal](#run-using-bare-metal)\n",
    "\n",
    "Details about Bare Metal method can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859bce34-5cf5-4df7-8b66-48dc56b47473",
   "metadata": {},
   "source": [
    "<a id=\"run-using-bare-metal\"></a> \n",
    "# Run Using Bare Metal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e33d4-8990-446b-8617-61049d5a9003",
   "metadata": {},
   "source": [
    "### 1. Create environment and install software packages\n",
    "\n",
    "**Opton 1: Using conda:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58a043-e979-4626-84b5-b087a30fcbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n anomaly_det_refkit python=3.9 ipykernel -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b06a05e-f2ee-45f4-a9ee-d6bf9485f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f9553",
   "metadata": {},
   "source": [
    "NOTE: Restart kernel and wait some seconds, and change kernel to conda env:anomaly_det_refkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d359d1-df30-4311-a350-4e37726a9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify anomaly_det_refkit environment is active\n",
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b445eb4",
   "metadata": {},
   "source": [
    "**Option 2: Using virtualenv:**\n",
    "\n",
    "```\n",
    "python3 -m venv anomaly_det_refkit\n",
    "source anomaly_det_refkit/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2fa260",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv anomaly_det_refkit  \n",
    "!anomaly_det_refkit/bin/python3 -m pip install ipykernel\n",
    "!anomaly_det_refkit/bin/python3 -m ipykernel install --user --name anomaly_det_refkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d4f23-2205-421b-a8b5-c9d9e2599a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8479d-bf70-4359-85bf-b2235aeff557",
   "metadata": {},
   "source": [
    "**NOTE:** Restart kernel and wait some seconds, and change kernel to anomaly_det_refkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324473fa-29d2-4d4b-a28e-9f443a549ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!anomaly_det_refkit/bin/pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e5c0fc-f0eb-4a16-ba25-c500ecef10c8",
   "metadata": {},
   "source": [
    "### 2. Download the dataset\n",
    "\n",
    "Download the mvtec dataset using Intel Model Zoo dataset download API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56394dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/IntelAI/models.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52909a9",
   "metadata": {},
   "source": [
    "Install dependencies and download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffef51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd models/datasets/dataset_api/\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b563ae2",
   "metadata": {},
   "source": [
    "**Note**: Run on terminal with anomaly_det_refkit virtual environment activated\n",
    "to review terms and conditions\n",
    "```\n",
    "cd models/datasets/dataset_api/\n",
    "./setup.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b560b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python dataset.py -n mvtec-ad --download -d ../../../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa893f57",
   "metadata": {},
   "source": [
    "Extract the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../../../\n",
    "!mkdir mvtec_dataset\n",
    "!tar -xf mvtec_anomaly_detection.tar.xz --directory mvtec_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24f317-c892-4db2-99a9-79fa5e9743d5",
   "metadata": {},
   "source": [
    "### 3. Select parameters and configurations\n",
    "\n",
    "Select the parameters and configurations in the [finetuning.yaml](configs/README.md) file.\n",
    "\n",
    "NOTE: \n",
    "When using SimSiam self supervised training, download the Sim-Siam weights based on ResNet50 model and place under simsiam directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6bd2a0-556b-47fa-b586-cd0d804959bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir simsiam\n",
    "!wget --directory-prefix=/simsiam/ https://dl.fbaipublicfiles.com/simsiam/models/100ep-256bs/pretrain/checkpoint_0099.pth.tar -o ./simsiam/checkpoint_0099.pth.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d27ab4",
   "metadata": {},
   "source": [
    "### 4. Running the end-to-end use case \n",
    "\n",
    "Using Transfer Learning Tool based fine-tuning:\n",
    "\n",
    "In finetuning.yaml, set **'fine_tune'** flag to true. If you downloaded the data from [DataSet](#DataSet) **change ./data/ to ./mvtec_dataset/** and set the pretrained/simsiam/cutpaste settings accordingly.\n",
    "Change other settings as intended in finetuning.yaml to run different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2322848-23a0-49a6-b4b0-88efbba33759",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONPATH=transfer-learning/\n",
    "!python anomaly_detection.py --config_file ./configs/finetuning.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e354ef5-79d8-4485-bd4f-e6ea80416847",
   "metadata": {},
   "source": [
    "<a id=\"expected-output\"></a> \n",
    "## Expected Output\n",
    "\n",
    "```\n",
    "+------------+------------------------+-------+--------------+\n",
    "|  Category  | Test set (Image count) | AUROC | Accuracy (%) |\n",
    "+------------+------------------------+-------+--------------+\n",
    "|   BOTTLE   |           83           | 99.92 |     98.8     |\n",
    "|   CABLE    |          150           | 94.36 |    88.67     |\n",
    "|  CAPSULE   |          132           | 95.33 |    87.12     |\n",
    "|   CARPET   |          117           | 91.65 |    83.76     |\n",
    "|    GRID    |           78           |  86.3 |    82.05     |\n",
    "|  HAZELNUT  |          110           | 99.25 |    97.27     |\n",
    "|  LEATHER   |          124           |  99.9 |    98.39     |\n",
    "| METAL_NUT  |          115           |  93.3 |    90.43     |\n",
    "|    PILL    |          167           | 96.02 |    86.83     |\n",
    "|   SCREW    |          160           |  83.3 |    81.88     |\n",
    "|    TILE    |          117           | 98.81 |    99.15     |\n",
    "| TOOTHBRUSH |           42           | 96.11 |     88.1     |\n",
    "| TRANSISTOR |          100           | 96.42 |     91.0     |\n",
    "|    WOOD    |           79           |  99.3 |    97.47     |\n",
    "|   ZIPPER   |          151           | 97.16 |    90.07     |\n",
    "+------------+------------------------+-------+--------------+\n",
    "```\n",
    "*Above results are on single node Dual socket 4th Generation Intel Xeon Scalable 8480+ (codenamed: Sapphire Rapids) Processor. 56 cores per socket, Intel® Turbo Boost Technology enabled, Intel® Hyper-Threading Technology enabled, 1024 GB memory (16x64GB), Configured Memory speed=4800 MT/s, INTEL SSDSC2BA012T4, CentOS Linux 8, BIOS=EGSDCRB.86B.WD.64.2022.29.7.13.1329, CPU Governor=performance, intel-extension-for-pytorch v2.0.0, torch 2.0.0, scikit-learn-intelex v2023.1.1, pandas 2.0.1. Configuration: precision=bfloat16, batch size=32, features extracted from pretrained resnet50v1.50 model.*\n",
    "\n",
    "\n",
    "<a id=\"summary-and-next-steps\"></a> \n",
    "## Summary and Next Steps\n",
    "\n",
    "\n",
    "The reference use case above demonstrates an Anomaly Detection approach using deep feature extraction and out-of-distrabution detection. It uses a tunable, modular workflow for fine-tuning the model & extractingits features, both of which uses the Intel® Transfer Learning Tool underneath. For optimal performance on Intel architecture, the scripts are also enabled with Intel extension for PyTorch, Intel extension for scikit-learn and has an option to run bfloat16 on 4th Gen Intel Xeon scalable processors using Intel® Advanced Matrix Extensions (Intel® AMX).\n",
    "\n",
    "### How to customize this use case\n",
    "Tunable configurations and parameters are exposed using yaml config files allowing users to change model training hyperparameters, datatypes, paths, and dataset settings without having to modify or search through the code.\n",
    "\n",
    "<a id=\"adopt-to-your-dataset\"></a> \n",
    "#### Adopt to your dataset\n",
    "\n",
    "This reference use case can be easily deployed on a different or customized dataset by simply arranging the images for training and testing in the following folder structure (Note that this approach only uses good images for training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d23a40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFREOwogICAgZGF0YXNldC0tPnRyYWluOwogICAgZGF0YXNldC0tPnRlc3Q7CiAgICB0cmFpbi0tPkdvb2Q7CiAgICB0ZXN0LS0+Y3JhY2s7CiAgICB0ZXN0LS0+Z29vZDsKICAgIHRlc3QtLT5qb2ludDsKICAgIHRlc3QtLT5kb3Q7CiAgICB0ZXN0LS0+b3RoZXJfYW5vbWFsaWVzOwo=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Code to create diagram.\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mm(graph):\n",
    "  graphbytes = graph.encode(\"ascii\")\n",
    "  base64_bytes = base64.b64encode(graphbytes)\n",
    "  base64_string = base64_bytes.decode(\"ascii\")\n",
    "  display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "mm(\"\"\"\n",
    "graph TD;\n",
    "    dataset-->train;\n",
    "    dataset-->test;\n",
    "    train-->Good;\n",
    "    test-->crack;\n",
    "    test-->good;\n",
    "    test-->joint;\n",
    "    test-->dot;\n",
    "    test-->other_anomalies;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b7ef15",
   "metadata": {},
   "source": [
    "For example, to run it for a [Marble Surface Anomaly Detection dataset](https://www.kaggle.com/datasets/wardaddy24/marble-surface-anomaly-detection-2) in Kaggle, download the dataset and update the train folder to only include the 'good' folder. Move the sub-folders with anomaly images in train folder to either the corresponding test folders or delete them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfca796",
   "metadata": {},
   "source": [
    "<a id=\"adopt-to-your-model\"></a> \n",
    "#### Adopt to your model\n",
    "\n",
    "#### 1. Change to a different pre-trained model from Torchvision:\n",
    "Change the 'model/name' variable in configs/finetuning.yaml to the intended model e.g.: resnet18\n",
    "\n",
    "For simsiam, download the Sim-Siam weights based on the new model and place it under the simsiam directory. If no pre-trained simsiam weights are available, fine-tuning will take time and have to be run for more epochs. \n",
    "Change other settings as intended in config.yaml to run different configurations. Then run the application using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONPATH=transfer-learning/\n",
    "!python anomaly_detection.py --config_file ./configs/finetuning.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d4815",
   "metadata": {},
   "source": [
    "#### 2. Plug-in your own pre-trained customized model:\n",
    "\n",
    "In finetuning.yaml, change 'fine_tune' flag to false and provide a custom model path under 'saved_model_path'.\n",
    "Change other settings as intended in config.yaml to run different configurations.\n",
    "\n",
    "To test the custom model with the MVTec AD dataset, add the preprocess flag to the dataset.py script to generate CSV files under all classes required for data loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e03b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python dataset.py -n mvtec-ad --download --preprocess -d ../../../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7834e980",
   "metadata": {},
   "source": [
    "Then run the application using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONPATH=transfer-learning/\n",
    "!python anomaly_detection.py --config_file ./configs/finetuning.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f3426",
   "metadata": {},
   "source": [
    "<a id=\"learn-more\"></a> \n",
    "## Learn More\n",
    "For more information or to read about other relevant workflow examples, see these guides and software resources:\n",
    "- [Intel® Transfer Learning Tool](https://github.com/IntelAI/transfer-learning)\n",
    "- [Anomaly Detection fine-tuning workflow using SimSiam and CutPaste techniques](https://github.com/IntelAI/transfer-learning/tree/main/workflows/vision_anomaly_detection)\n",
    "- [Intel® AI Analytics Toolkit (AI Kit)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html)\n",
    "- [Intel® Extension for PyTorch](https://intel.github.io/intel-extension-for-pytorch/)\n",
    "- [Intel® Extension for Scikit-learn](https://www.intel.com/content/www/us/en/developer/tools/oneapi/scikit-learn.html#gs.x609e4)\n",
    "- [Intel® Neural Compressor](https://github.com/intel/neural-compressor)\n",
    "\n",
    "<a id=\"support\"></a> \n",
    "## Support\n",
    "If you have any questions with this workflow, want help with troubleshooting, want to report a bug or submit enhancement requests, please submit a GitHub issue.\n",
    "\n",
    "---\n",
    "\n",
    "\\*Other names and brands may be claimed as the property of others.\n",
    "[Trademarks](https://www.intel.com/content/www/us/en/legal/trademarks.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anomaly_det_refkit]",
   "language": "python",
   "name": "conda-env-anomaly_det_refkit-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
